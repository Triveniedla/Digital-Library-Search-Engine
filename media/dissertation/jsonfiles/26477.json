{
  "contributor_author": "Capra, Miranda Galadriel",
  "contributor_committeechair": "Smith-Jackson, Tonya L.",
  "contributor_committeemember": [
    "Nussbaum, Maury A.",
    "Kleiner, Brian M.",
    "Burton, John K.",
    "Hartson, H. Rex"
  ],
  "contributor_department": "Industrial and Systems Engineering",
  "date_accessioned": "2014-03-14T20:08:21Z",
  "date_adate": "2006-04-05",
  "date_available": "2014-03-14T20:08:21Z",
  "date_issued": "2006-03-13",
  "date_rdate": "2006-04-05",
  "date_sdate": "2006-03-22",
  "degree_grantor": "Virginia Polytechnic Institute and State University",
  "degree_level": "doctoral",
  "degree_name": "PhD",
  "description_abstract": "Previous usability evaluation method (UEM) comparison studies have noted an evaluator effect on problem detection in heuristic evaluation, with evaluators differing in problems found and problem severity judgments. There have been few studies of the evaluator effect in usability testing (UT), task-based testing with end-users. UEM comparison studies focus on counting usability problems detected, but we also need to assess the content of usability problem descriptions (UPDs) to more fully measure evaluation effectiveness. The goals of this research were to develop UPD guidelines, explore the evaluator effect in UT, and evaluate the usefulness of the guidelines for grading UPD content.<P>  Ten guidelines for writing UPDs were developed by consulting usability practitioners through two questionnaires and a card sort. These guidelines are (briefly): be clear and avoid jargon, describe problem severity, provide backing data, describe problem causes, describe user actions, provide a solution, consider politics and diplomacy, be professional and scientific, describe your methodology, and help the reader sympathize with the user. A fourth study compared usability reports collected from 44 evaluators, both practitioners and graduate students, watching the same 10-minute UT session recording. Three judges measured problem detection for each evaluator and graded the reports for following 6 of the UPD guidelines.<P>  There was support for existence of an evaluator effect, even when watching pre-recorded sessions, with low to moderate individual thoroughness of problem detection across all/severe problems (22%/34%), reliability of problem detection (37%/50%) and reliability of severity judgments (57% for severe ratings). Practitioners received higher grades averaged across the 6 guidelines than students did, suggesting that the guidelines may be useful for grading reports. The grades for the guidelines were not correlated with thoroughness, suggesting that the guideline grades complement measures of problem detection.<P>  A simulation of evaluators working in groups found a 34% increase in severe problems found by adding a second evaluator. The simulation also found that thoroughness of individual evaluators would have been overestimated if the study had included a small number of evaluators. The final recommendations are to use multiple evaluators in UT, and to assess both problem detection and description when measuring evaluation effectiveness.",
  "description_provenance": [
    "Author Email: mcapra-thesis@thecapras.org",
    "Advisor Email: smithjack@vt.edu",
    "Advisor Email: nussbaum@vt.edu",
    "Advisor Email: bkleiner@vt.edu",
    "Advisor Email: jburton@vt.edu",
    "Advisor Email: hartson@vt.edu",
    "Made available in DSpace on 2014-03-14T20:08:21Z (GMT). No. of bitstreams: 1 MCapra_dissertation.pdf: 2288429 bytes, checksum: 9af930d08ab3a39b30b28a04fd26e948 (MD5)   Previous issue date: 2006-03-13"
  ],
  "handle": "26477",
  "identifier_other": "etd-03222006-201913",
  "identifier_sourceurl": "http://scholar.lib.vt.edu/theses/available/etd-03222006-201913/",
  "identifier_uri": "http://hdl.handle.net/10919/26477",
  "publisher": "Virginia Tech",
  "relation_haspart": "MCapra_dissertation.pdf",
  "rights": "I hereby certify that, if appropriate, I have obtained and attached hereto a written permission statement from the owner(s) of each third party copyrighted matter to be included in my thesis, dissertation, or project report, allowing distribution as specified below.  I certify that the version I submitted is the same as that approved by my advisory committee.  I hereby grant to Virginia Tech or its agents the non-exclusive license to archive and make accessible, under the conditions specified below, my thesis, dissertation, or project report in whole or in part in all forms of media, now or hereafter known.  I retain all other ownership rights to the copyright of the thesis, dissertation or project report.  I also retain the right to use in future works (such as articles or books) all or part of this thesis, dissertation, or project report.",
  "subject": [
    "thoroughness",
    "reliability",
    "evaluator effect",
    "usability testing",
    "formative usability evaluation methods",
    "problem descriptions"
  ],
  "title": "Usability Problem Description and the Evaluator Effect in Usability Testing",
  "type": "Dissertation"
}